{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface[吾輩]\tbase[吾輩]\tpos[名詞]\tpos1[代名詞]\n",
      "surface[は]\tbase[は]\tpos[助詞]\tpos1[係助詞]\n",
      "surface[ここ]\tbase[ここ]\tpos[名詞]\tpos1[代名詞]\n",
      "surface[で]\tbase[で]\tpos[助詞]\tpos1[格助詞]\n",
      "surface[始め]\tbase[始める]\tpos[動詞]\tpos1[自立]\n",
      "surface[て]\tbase[て]\tpos[助詞]\tpos1[接続助詞]\n",
      "surface[人間]\tbase[人間]\tpos[名詞]\tpos1[一般]\n",
      "surface[という]\tbase[という]\tpos[助詞]\tpos1[格助詞]\n",
      "surface[もの]\tbase[もの]\tpos[名詞]\tpos1[非自立]\n",
      "surface[を]\tbase[を]\tpos[助詞]\tpos1[格助詞]\n",
      "surface[見]\tbase[見る]\tpos[動詞]\tpos1[自立]\n",
      "surface[た]\tbase[た]\tpos[助動詞]\tpos1[*]\n",
      "surface[。]\tbase[。]\tpos[記号]\tpos1[句点]\n"
     ]
    }
   ],
   "source": [
    "'''40. 係り受け解析結果の読み込み（形態素）Permalink\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）\n",
    "をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，\n",
    "各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ'''\n",
    "\n",
    "filename = 'data/ch5/neko.txt.cabocha'\n",
    "\n",
    "class Morph():\n",
    "    def __init__(self, line):\n",
    "        surface, details = line.rstrip().split('\\t')\n",
    "        details = details.split(',')\n",
    "        base, pos, pos1 = details[6], details[0], details[1]\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    def __str__(self):\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(self.surface, self.base, self.pos, self.pos1)\n",
    "    \n",
    "    \n",
    "def read_cabocha(filename):\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if line == \"EOS\\n\":\n",
    "                if len(sentence):\n",
    "                    yield sentence\n",
    "                sentence = []\n",
    "            else:\n",
    "                if line[0] == '*':\n",
    "                    continue\n",
    "                sentence.append(Morph(line))\n",
    "\n",
    "for i, morphs in enumerate(read_cabocha(filename)):\n",
    "    if i == 4:\n",
    "        for morph in morphs:\n",
    "            print(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:[0]何でも\tsrc:[]\tdst: 1    \n",
      "index:[1]薄暗い\tsrc:[0]\tdst: 3    \n",
      "index:[2]じめじめした\tsrc:[]\tdst: 3    \n",
      "index:[3]所で\tsrc:[1, 2]\tdst: 5    \n",
      "index:[4]ニャーニャー\tsrc:[]\tdst: 5    \n",
      "index:[5]泣いて\tsrc:[3, 4]\tdst: 7    \n",
      "index:[6]いた事だけは\tsrc:[]\tdst: 7    \n",
      "index:[7]記憶している。\tsrc:[5, 6]\tdst: -1   \n"
     ]
    }
   ],
   "source": [
    "'''41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．\n",
    "このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），\n",
    "係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．\n",
    "さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，\n",
    "8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．'''\n",
    "from collections import OrderedDict\n",
    "\n",
    "  \n",
    "class Chunk():\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "    def show_surface(self):\n",
    "        return \"\".join([morph.surface for morph in self.morphs])  \n",
    "    def show_dst(self):\n",
    "        return self.dst \n",
    "    def show_srcs(self):\n",
    "        return self.srcs\n",
    "        \n",
    "def read_cabocha(filename):\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        idx = -1\n",
    "        dsts = []\n",
    "        chunks = []\n",
    "        for i,line in enumerate(f):\n",
    "            # if EOS appears, this sentence is end\n",
    "            if line == \"EOS\\n\":\n",
    "                if len(chunks):\n",
    "                    for index,d in enumerate(dsts):\n",
    "                        chunks[d].srcs.append(index)\n",
    "                    yield chunks\n",
    "                    dsts = []\n",
    "                    chunks = []\n",
    "                    \n",
    "            # parsing result, store index and dst at this point\n",
    "            elif line[0] == '*':\n",
    "                cols = line.split(' ')\n",
    "                idx = int(cols[1])\n",
    "                dst = int(cols[2][:-1])\n",
    "                \n",
    "                chunks.append(Chunk())\n",
    "                chunks[-1].dst = dst\n",
    "\n",
    "                if dst!=-1:  # if dst=-1, means end of section\n",
    "                    dsts.append(dst)                     \n",
    "                    \n",
    "            # we have to store all stuff after parsing result appeared.\n",
    "            else:\n",
    "                chunks[-1].morphs.append(Morph(line))\n",
    "                \n",
    "\n",
    "for i, chunks in enumerate(read_cabocha(filename)):\n",
    "    if i == 3:\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            print(\"index:[{}]{}\\tsrc:{}\\tdst: {:<5}\".format(j, chunk.show_surface(),chunk.show_srcs(),chunk.show_dst()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　\t猫である。\n",
      "吾輩は\t猫である。\n",
      "--------------------------------------------------------------------------------\n",
      "名前は\t無い。\n",
      "まだ\t無い。\n",
      "--------------------------------------------------------------------------------\n",
      "　どこで\t生れたか\n",
      "生れたか\tつかぬ。\n",
      "とんと\tつかぬ。\n",
      "見当が\tつかぬ。\n",
      "--------------------------------------------------------------------------------\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している。\n",
      "いた事だけは\t記憶している。\n",
      "--------------------------------------------------------------------------------\n",
      "吾輩は\t見た。\n",
      "ここで\t始めて\n",
      "始めて\t人間という\n",
      "人間という\tものを\n",
      "ものを\t見た。\n",
      "--------------------------------------------------------------------------------\n",
      "しかも\t種族であったそうだ。\n",
      "あとで\t聞くと\n",
      "聞くと\t種族であったそうだ。\n",
      "それは\t種族であったそうだ。\n",
      "書生という\t人間中で\n",
      "人間中で\t種族であったそうだ。\n",
      "一番\t獰悪な\n",
      "獰悪な\t種族であったそうだ。\n",
      "--------------------------------------------------------------------------------\n",
      "この\t書生というのは\n",
      "書生というのは\t話である。\n",
      "時々\t捕えて\n",
      "我々を\t捕えて\n",
      "捕えて\t煮て\n",
      "煮て\t食うという\n",
      "食うという\t話である。\n",
      "--------------------------------------------------------------------------------\n",
      "しかし\t思わなかった。\n",
      "その\t当時は\n",
      "当時は\tなかったから\n",
      "何という\t考も\n",
      "考も\tなかったから\n",
      "なかったから\t思わなかった。\n",
      "別段\t恐し\n",
      "恐し\t思わなかった。\n",
      "いとも\t思わなかった。\n",
      "--------------------------------------------------------------------------------\n",
      "ただ\t載せられて\n",
      "彼の\t掌に\n",
      "掌に\t載せられて\n",
      "載せられて\t持ち上げられた\n",
      "スーと\t持ち上げられた\n",
      "持ち上げられた\t時\n",
      "時\tフワフワした\n",
      "何だか\tフワフワした\n",
      "フワフワした\t感じが\n",
      "感じが\tあったばかりである。\n",
      "--------------------------------------------------------------------------------\n",
      "掌の\t上で\n",
      "上で\t落ちついて\n",
      "少し\t落ちついて\n",
      "落ちついて\t見たのが\n",
      "書生の\t顔を\n",
      "顔を\t見たのが\n",
      "見たのが\t人間という\n",
      "いわゆる\t人間という\n",
      "人間という\tものの\n",
      "ものの\t見始であろう。\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．\n",
    "ただし，句読点などの記号は出力しないようにせよ．'''\n",
    "        \n",
    "for i,chunks in enumerate(read_cabocha(filename)):\n",
    "    if i==10: break\n",
    "    for chunk in chunks:\n",
    "        if chunk.dst != -1:\n",
    "            src = chunk.show_surface()  \n",
    "            dst = chunks[chunk.dst].show_surface() # dependency \n",
    "            print('{}\\t{}'.format(src, dst))\n",
    "    print('-'*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　どこで\t生れたか\n",
      "見当が\tつかぬ。\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している。\n",
      "吾輩は\t見た。\n",
      "ここで\t始めて\n",
      "ものを\t見た。\n",
      "あとで\t聞くと\n",
      "我々を\t捕えて\n",
      "掌に\t載せられて\n",
      "スーと\t持ち上げられた\n",
      "時\tフワフワした\n",
      "感じが\tあったばかりである。\n",
      "上で\t落ちついて\n",
      "顔を\t見たのが\n",
      "ものの\t見始であろう。\n"
     ]
    }
   ],
   "source": [
    "'''43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．\n",
    "ただし，句読点などの記号は出力しないようにせよ．'''\n",
    "\n",
    "    \n",
    "class Chunk():\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "    def show_surface(self):\n",
    "        return \"\".join([morph.surface for morph in self.morphs])  \n",
    "    def show_dst(self):\n",
    "        return self.dst \n",
    "    def show_srcs(self):\n",
    "        return self.srcs  \n",
    "    def check_pos(self, specified):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == specified:\n",
    "                return True\n",
    "        return False    \n",
    "\n",
    "for i,chunks in enumerate(read_cabocha(filename)):\n",
    "    if i==10: break\n",
    "    for chunk in chunks:\n",
    "        if chunk.dst != -1:\n",
    "            # contains noun and dependency section is verb\n",
    "            if chunk.check_pos(\"名詞\") and chunks[chunk.dst].check_pos(\"動詞\"):\n",
    "                src = chunk.show_surface()\n",
    "                dst = chunks[chunk.dst].show_surface()\n",
    "                print('{}\\t{}'.format(src, dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph G {\n",
      "\"どこで\" -> \"生れたか\";\n",
      "\"生れたか\" -> \"つかぬ\";\n",
      "\"とんと\" -> \"つかぬ\";\n",
      "\"見当が\" -> \"つかぬ\";\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "# !pip install pydot\n",
    "# !apt-get install graphviz  --yes\n",
    "# !conda install graphviz --yes\n",
    "\n",
    "'''44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．\n",
    "可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．\n",
    "また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．'''\n",
    "\n",
    "import pydot\n",
    "\n",
    "for i, chunks in enumerate(read_cabocha(filename)):\n",
    "    if i == 2:\n",
    "        target = chunks\n",
    "        break\n",
    "\n",
    "pairs = []\n",
    "for morph in target:\n",
    "    if int(morph.dst) != -1:\n",
    "        preText = ''.join([mo.surface if mo.pos != '記号' else '' for mo in morph.morphs])\n",
    "        postText = ''.join([mo.surface if mo.pos != '記号' else '' for mo in target[int(morph.dst)].morphs])\n",
    "        pairs.append([preText, postText])\n",
    "        \n",
    "# pairs = [['dokode','umaretaka'],['umaretaka','tsukanu'],['tonnto','tsukanu'],['miatariga','tsukanu']]\n",
    "\n",
    "g = pydot.graph_from_edges(pairs,  directed=True)\n",
    "print(g)\n",
    "# g.write_png('data/ch5/ans44.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい．\n",
    "動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "述語に係る助詞を格とする\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える．\n",
    "この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，\n",
    "「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "始める  で\n",
    "見る    は を'''\n",
    "\n",
    "class Chunk():\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "        \n",
    "    def show_surface(self):\n",
    "        return \"\".join([morph.surface for morph in self.morphs])  \n",
    "    \n",
    "    def show_dst(self):\n",
    "        return self.dst \n",
    "    \n",
    "    def show_srcs(self):\n",
    "        return self.srcs \n",
    "    \n",
    "    def check_pos(self, specified):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == specified:\n",
    "                return True\n",
    "        return False    \n",
    "    \n",
    "    def get_morphs_pos(self, pos, pos1=''):\n",
    "        tars = []\n",
    "        if len(pos1):\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos and morph.pos1 == pos1:\n",
    "                    tars.append(morph)\n",
    "        else:\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    tars.append(morph)\n",
    "        return tars\n",
    "\n",
    "\n",
    "fname_out = 'data/ch5/neko_aux.txt'\n",
    "with open(fname_out, mode='w',encoding='utf-8') as f:\n",
    "    for i,chunks in enumerate(read_cabocha(filename)):\n",
    "#         if i==3: \n",
    "        for chunk in chunks:\n",
    "            auxs = []            \n",
    "            verbs = chunk.get_morphs_pos(\"動詞\")\n",
    "            # if verb exists in the section\n",
    "            if verbs:\n",
    "                for src in chunk.srcs:\n",
    "                    aux = chunks[src].get_morphs_pos(\"助詞\")            \n",
    "                    if len(aux) > 0:\n",
    "                        auxs.append(aux[-1])\n",
    "                if len(auxs) < 1:\n",
    "                    continue\n",
    "#                 print(\"{}\\t{}\\n\".format(verbs[0].base,\" \".join(sorted([aux.surface for aux in auxs]))))\n",
    "                f.write(\"{}\\t{}\\n\".format(verbs[0].base,\" \".join(sorted([aux.surface for aux in auxs]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\t　どこで\n",
      "つく\tか が\t生れたか 見当が\n",
      "泣く\tで\t所で\n",
      "する\tて は\t泣いて いた事だけは\n",
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n",
      "思う\tから\tなかったから\n",
      "載せる\tに\t掌に\n",
      "持ち上げる\tて と\t載せられて スーと\n",
      "ある\tが\t感じが\n",
      "落ちつく\tで\t上で\n",
      "見る\tて を\t落ちついて 顔を\n",
      "見る\tの\tものの\n"
     ]
    }
   ],
   "source": [
    "'''46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ\n",
    "．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える．\n",
    "この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，\n",
    "「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを'''\n",
    "\n",
    "\n",
    "\n",
    "fname_out = 'data/ch5/neko_aux.txt'\n",
    "\n",
    "for i,chunks in enumerate(read_cabocha(filename)):\n",
    "    if i==10: break \n",
    "    for chunk in chunks:\n",
    "        auxs = []       \n",
    "        aux_sent = []\n",
    "        verbs = chunk.get_morphs_pos(\"動詞\")\n",
    "        # if verb exists in the section\n",
    "        if verbs:\n",
    "            for src in chunk.srcs:\n",
    "                aux = chunks[src].get_morphs_pos(\"助詞\")\n",
    "                if len(aux) > 0:\n",
    "                    auxs.append(aux[-1])\n",
    "                    aux_sent.append(chunks[src].show_surface())\n",
    "            if len(auxs) < 1:\n",
    "                continue\n",
    "            print(\"{}\\t{}\\t{}\".format(verbs[0].base,\" \".join(\n",
    "                sorted([aux.surface for aux in auxs])),\" \".join(aux_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "コーパス中で頻出する述語と助詞パターン'''\n",
    "    \n",
    "class Chunk():\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "        \n",
    "    def show_surface(self):\n",
    "        return \"\".join([morph.surface for morph in self.morphs])  \n",
    "    \n",
    "    def show_dst(self):\n",
    "        return self.dst \n",
    "    \n",
    "    def show_srcs(self):\n",
    "        return self.srcs \n",
    "    \n",
    "    def check_pos(self, specified):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == specified:\n",
    "                return True\n",
    "        return False    \n",
    "    \n",
    "    def get_morphs_pos(self, pos, pos1=''):\n",
    "        tars = []\n",
    "        if len(pos1):\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos and morph.pos1 == pos1:\n",
    "                    tars.append(morph)\n",
    "        else:\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    tars.append(morph)\n",
    "        return tars\n",
    "\n",
    "    def get_sa_verb(self):\n",
    "        tars = []    \n",
    "        for i in range(len(self.morphs[0:-1])):\n",
    "            if self.morphs[i].pos == '名詞' and self.morphs[i].pos1 == 'サ変接続':\n",
    "                if self.morphs[i+1].surface == 'を' and self.morphs[i+1].pos == '助詞':\n",
    "                    tars.append([self.morphs[i].surface + self.morphs[i+1].surface])    \n",
    "        return tars\n",
    "    \n",
    "\n",
    "with open('data/ch5/47.txt','w', encoding='utf-8') as f:\n",
    "    for i,chunks in enumerate(read_cabocha(filename)):\n",
    "        for chunk in chunks:\n",
    "            auxs = []       \n",
    "            aux_sent = []\n",
    "            verbs = chunk.get_morphs_pos(\"動詞\")\n",
    "            if len(verbs) < 1:\n",
    "                continue\n",
    "\n",
    "            for src in chunk.srcs:\n",
    "                aux = chunks[src].get_morphs_pos(\"助詞\")\n",
    "                if len(aux):\n",
    "                    auxs.append(aux[-1]) #take the last aux\n",
    "                    aux_sent.append(chunks[src])\n",
    "            if len(auxs) < 1:\n",
    "                continue\n",
    "\n",
    "            for aux_part in aux_sent:\n",
    "                sa_verb = aux_part.get_sa_verb()\n",
    "                if len(sa_verb):\n",
    "                    aux_sent.remove(aux_sent[-1])\n",
    "                    auxs.remove(auxs[-1])\n",
    "\n",
    "            if len(sa_verb) < 1:\n",
    "                continue\n",
    "\n",
    "            s = sorted(zip([aux.surface for aux in auxs],[sent.show_surface() for sent in aux_sent]), key=lambda x: x[0])\n",
    "            aux_vs = [aux_v[0] for aux_v in s]\n",
    "            aux_sent = [aux_v[1] for aux_v in s]\n",
    "\n",
    "    #         print(\"{} {} {}\\n\".format(\"\".join(sa_verb[0])+verbs[0].base,\" \".join(aux_vs),\" \".join(aux_sent)))\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(\"\".join(sa_verb[0])+verbs[0].base,\" \".join(aux_vs),\" \".join(aux_sent)))\n",
    "\n",
    "\n",
    "# !cut --fields=1 data/ch5/47.txt | sort | uniq --count | sort --numeric-sort --reverse >  data/ch5/47_col1.txt\n",
    "# !cut --fields=1,2 data/ch5/47.txt | sort | uniq --count | sort --numeric-sort --reverse >  data/ch5/47_col12.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は-> 猫である。\n",
      "\n",
      "猫である。\n",
      "\n",
      "名前は-> 無い。\n",
      "\n",
      "　どこで-> 生れたか-> つかぬ。\n",
      "\n",
      "見当が-> つかぬ。\n",
      "\n",
      "何でも-> 薄暗い-> 所で-> 泣いて-> 記憶している。\n",
      "\n",
      "所で-> 泣いて-> 記憶している。\n",
      "\n",
      "ニャーニャー-> 泣いて-> 記憶している。\n",
      "\n",
      "いた事だけは-> 記憶している。\n",
      "\n",
      "記憶している。\n",
      "\n",
      "吾輩は-> 見た。\n",
      "\n",
      "ここで-> 始めて-> 人間という-> ものを-> 見た。\n",
      "\n",
      "人間という-> ものを-> 見た。\n",
      "\n",
      "ものを-> 見た。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "各文節は（表層形の）形態素列で表現する\n",
    "パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た'''\n",
    "      \n",
    "for i,chunks in enumerate(read_cabocha(filename)):\n",
    "    if i==5: break\n",
    "    for chunk in chunks:\n",
    "        noun = chunk.get_morphs_pos(\"名詞\")\n",
    "        if len(noun):\n",
    "            dst = chunk.dst\n",
    "            print(chunk.show_surface(),end='')\n",
    "            while dst!=-1:\n",
    "                print('->',chunks[dst].show_surface(),end='')\n",
    "                dst = chunks[dst].dst\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yで -> 始めて -> 人間という -> ものを | 見た。\n",
      "Xは | Yという -> ものを | 見た。\n",
      "Xは | Yを | 見た。\n",
      "Xで -> 始めて -> Y\n",
      "Xで -> 始めて -> 人間という -> Y\n",
      "Xという -> Y\n"
     ]
    }
   ],
   "source": [
    "'''49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．\n",
    "ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: \n",
    "    文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y'''\n",
    "\n",
    "'''48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "各文節は（表層形の）形態素列で表現する\n",
    "パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た'''\n",
    "\n",
    "\n",
    "class Chunk():\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "        \n",
    "    def show_surface(self):\n",
    "        return \"\".join([morph.surface for morph in self.morphs])  \n",
    "    \n",
    "    def show_dst(self):\n",
    "        return self.dst \n",
    "    \n",
    "    def show_srcs(self):\n",
    "        return self.srcs \n",
    "    \n",
    "    def check_pos(self, specified):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == specified:\n",
    "                return True\n",
    "        return False    \n",
    "    \n",
    "    def get_morphs_pos(self, pos, pos1=''):\n",
    "        tars = []\n",
    "        if len(pos1):\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos and morph.pos1 == pos1:\n",
    "                    tars.append(morph)\n",
    "        else:\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    tars.append(morph)\n",
    "        return tars\n",
    "    \n",
    "    def convert_symbol(self, symbol):\n",
    "        result = []\n",
    "        for morphs in self.morphs:\n",
    "            if morphs.pos == \"名詞\" and morphs.pos!='記号':\n",
    "                result += symbol\n",
    "            else:\n",
    "                result += morphs.surface\n",
    "        return \"\".join(result)\n",
    "    \n",
    "def collision_check(chunks, noun1, noun2):\n",
    "    route1 = [noun1]\n",
    "    route2 = [noun2]\n",
    "\n",
    "    dst = noun1.dst\n",
    "    while dst!=-1:\n",
    "        route1.append(chunks[dst])\n",
    "        dst = chunks[dst].dst\n",
    "\n",
    "    dst = noun2.dst        \n",
    "    while dst!=-1:\n",
    "        route2.append(chunks[dst])\n",
    "        dst = chunks[dst].dst    \n",
    "\n",
    "    if noun2 in route1:\n",
    "        return -1\n",
    "    else:\n",
    "        s = [c.dst for c in route2 if chunks[c.dst] in route1][0]\n",
    "        return s    \n",
    "\n",
    "def print_path(path, chunks, noun, symbol, collapse):\n",
    "    print(noun.convert_symbol(symbol),end='')\n",
    "    dst = noun.dst\n",
    "    if path =='y2x':\n",
    "        while dst != collapse:\n",
    "            print(' -> ' + chunks[dst].show_surface(),end='')\n",
    "            dst = chunks[dst].dst\n",
    "        print(' | ',end='')   \n",
    "    else:\n",
    "        while dst != -1:   \n",
    "            if chunks[dst] == collapse:\n",
    "                result = chunks[dst].convert_symbol('Y')\n",
    "                print(' -> ' + result[:result.index('Y') + 1])\n",
    "                break\n",
    "            print(' -> ' +  chunks[dst].show_surface(),end='')   \n",
    "            dst = chunks[dst].dst         \n",
    "\n",
    "def get_path(chunks):\n",
    "    nouns = [chunk for chunk in chunks if \"名詞\" in [m.pos for m in chunk.morphs]]     \n",
    "    if len(nouns) > 1:\n",
    "        for i,noun1 in enumerate(nouns[:-1]):\n",
    "            for j,noun2 in enumerate(nouns[i+1:]):\n",
    "                collapse = collision_check(chunks, noun1, noun2)\n",
    "\n",
    "                if collapse == -1:\n",
    "                    print_path('x2y', chunks, noun1, 'X', noun2)\n",
    "                    \n",
    "                else:\n",
    "                    print_path('y2x', chunks, noun1, 'X',collapse)\n",
    "                    print_path('y2x', chunks, noun2, 'Y',collapse)\n",
    "                    print(chunks[collapse].show_surface())\n",
    "    \n",
    "filename = 'data/ch5/neko.txt.cabocha'\n",
    "for i,chunks in enumerate(read_cabocha(filename)):\n",
    "    if i==4: \n",
    "        get_path(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
